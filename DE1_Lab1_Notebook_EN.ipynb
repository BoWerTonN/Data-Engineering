{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DE1 — Lab 1: PySpark Warmup and Reading Plans\n",
        "> Author : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n",
        "---\n",
        "\n",
        "This notebook is the **student deliverable**. Execute all cells and attach evidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Imports and Spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyspark'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession, functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPython:\u001b[39m\u001b[33m\"\u001b[39m, sys.version)\n\u001b[32m      4\u001b[39m spark = SparkSession.builder.appName(\u001b[33m\"\u001b[39m\u001b[33mde1-lab1\u001b[39m\u001b[33m\"\u001b[39m).getOrCreate()\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
          ]
        }
      ],
      "source": [
        "import os, sys, datetime, pathlib\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "print(\"Python:\", sys.version)\n",
        "spark = SparkSession.builder.appName(\"de1-lab1\").getOrCreate()\n",
        "print(\"Spark:\", spark.version)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the CSV inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows: 2700\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- value: double (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            "\n",
            "+---+-----------+-----+----------------------------------------------------------------------------+\n",
            "|id |category   |value|text                                                                        |\n",
            "+---+-----------+-----+----------------------------------------------------------------------------+\n",
            "|0  |toys       |48.47|metrics ui data elt row columnar reduce warehouse shuffle join spark elt    |\n",
            "|1  |books      |39.9 |metrics row lake aggregate columnar data reduce row columnar filter         |\n",
            "|2  |grocery    |7.96 |lake join partition scala elt data                                          |\n",
            "|3  |electronics|5.15 |spark scala elt filter join columnar lake lake plan warehouse columnar spark|\n",
            "|4  |toys       |44.87|aggregate metrics row row filter lake map metrics columnar spark            |\n",
            "+---+-----------+-----+----------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "src_a = \"/mnt/c/Users/daryl/lab1/lab1_dataset_a.csv\"\n",
        "src_b = \"/mnt/c/Users/daryl/lab1/lab1_dataset_b.csv\"\n",
        "df_a = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(src_a)\n",
        "df_b = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(src_b)\n",
        "df = df_a.unionByName(df_b)\n",
        "df.cache()\n",
        "print(\"Rows:\", df.count())\n",
        "df.printSchema()\n",
        "df.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Top‑N with **RDD** API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('lake', 1215),\n",
              " ('scala', 1200),\n",
              " ('elt', 1199),\n",
              " ('metrics', 1190),\n",
              " ('row', 1183),\n",
              " ('join', 1169),\n",
              " ('warehouse', 1168),\n",
              " ('shuffle', 1160),\n",
              " ('ui', 1145),\n",
              " ('aggregate', 1144)]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# RDD pipeline: tokenize 'text' column and count tokens\n",
        "rdd = df.select(\"text\").rdd.flatMap(lambda row: (row[0] or \"\").lower().split())\n",
        "pair = rdd.map(lambda t: (t, 1))\n",
        "counts = pair.reduceByKey(lambda a,b: a+b)\n",
        "top_rdd = counts.sortBy(lambda kv: (-kv[1], kv[0])).take(10)\n",
        "top_rdd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pathlib' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save as CSV (token,count)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mpathlib\u001b[49m.Path(\u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m).mkdir(exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33moutputs/top10_rdd.csv\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m,encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m     f.write(\u001b[33m\"\u001b[39m\u001b[33mtoken,count\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'pathlib' is not defined"
          ]
        }
      ],
      "source": [
        "# Save as CSV (token,count)\n",
        "pathlib.Path(\"outputs\").mkdir(exist_ok=True)\n",
        "with open(\"outputs/top10_rdd.csv\",\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(\"token,count\\n\")\n",
        "    for t,c in top_rdd:\n",
        "        f.write(f\"{t},{c}\\n\")\n",
        "print(\"Wrote outputs/top10_rdd.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RDD plan — evidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved proof/plan_rdd.txt\n"
          ]
        }
      ],
      "source": [
        "# Trigger an action and record a textual plan for evidence\n",
        "_ = counts.count()\n",
        "plan_rdd = df._jdf.queryExecution().executedPlan().toString()\n",
        "pathlib.Path(\"proof\").mkdir(exist_ok=True)\n",
        "with open(\"proof/plan_rdd.txt\",\"w\") as f:\n",
        "    f.write(str(datetime.datetime.now()) + \"\\n\\n\")\n",
        "    f.write(plan_rdd)\n",
        "print(\"Saved proof/plan_rdd.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Top‑N with **DataFrame** API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|token    |count|\n",
            "+---------+-----+\n",
            "|lake     |1215 |\n",
            "|scala    |1200 |\n",
            "|elt      |1199 |\n",
            "|metrics  |1190 |\n",
            "|row      |1183 |\n",
            "|join     |1169 |\n",
            "|warehouse|1168 |\n",
            "|shuffle  |1160 |\n",
            "|ui       |1145 |\n",
            "|aggregate|1144 |\n",
            "+---------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote outputs/top10_df.csv\n"
          ]
        }
      ],
      "source": [
        "tokens = F.explode(F.split(F.lower(F.col(\"text\")), \"\\\\s+\")).alias(\"token\")\n",
        "df_tokens = df.select(tokens).where(F.col(\"token\") != \"\")\n",
        "agg_df = df_tokens.groupBy(\"token\").agg(F.count(\"*\").alias(\"count\"))\n",
        "top_df = agg_df.orderBy(F.desc(\"count\"), F.asc(\"token\")).limit(10)\n",
        "top_df.show(truncate=False)\n",
        "top_df.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"outputs/top10_df_tmp\")\n",
        "# move single part file to stable path\n",
        "import glob, shutil\n",
        "part = glob.glob(\"outputs/top10_df_tmp/part*\")[0]\n",
        "shutil.copy(part, \"outputs/top10_df.csv\")\n",
        "print(\"Wrote outputs/top10_df.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataFrame plan — evidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved proof/plan_df.txt\n"
          ]
        }
      ],
      "source": [
        "plan_df = top_df._jdf.queryExecution().executedPlan().toString()\n",
        "with open(\"proof/plan_df.txt\",\"w\") as f:\n",
        "    f.write(str(datetime.datetime.now()) + \"\\n\\n\")\n",
        "    f.write(plan_df)\n",
        "print(\"Saved proof/plan_df.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Projection experiment: `select(\"*\")` vs minimal projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan (9)\n",
            "+- HashAggregate (8)\n",
            "   +- Exchange (7)\n",
            "      +- HashAggregate (6)\n",
            "         +- InMemoryTableScan (1)\n",
            "               +- InMemoryRelation (2)\n",
            "                     +- Union (5)\n",
            "                        :- Scan csv  (3)\n",
            "                        +- Scan csv  (4)\n",
            "\n",
            "\n",
            "(1) InMemoryTableScan\n",
            "Output [2]: [category#18, value#19]\n",
            "Arguments: [category#18, value#19]\n",
            "\n",
            "(2) InMemoryRelation\n",
            "Arguments: [id#17, category#18, value#19, text#20], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "\n",
            "(3) Scan csv \n",
            "Output [4]: [id#17, category#18, value#19, text#20]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/mnt/c/Users/daryl/lab1/lab1_dataset_a.csv]\n",
            "ReadSchema: struct<id:int,category:string,value:double,text:string>\n",
            "\n",
            "(4) Scan csv \n",
            "Output [4]: [id#38, category#39, value#40, text#41]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/mnt/c/Users/daryl/lab1/lab1_dataset_b.csv]\n",
            "ReadSchema: struct<id:int,category:string,value:double,text:string>\n",
            "\n",
            "(5) Union\n",
            "\n",
            "(6) HashAggregate\n",
            "Input [2]: [category#18, value#19]\n",
            "Keys [1]: [category#18]\n",
            "Functions [1]: [partial_sum(value#19)]\n",
            "Aggregate Attributes [1]: [sum#752]\n",
            "Results [2]: [category#18, sum#753]\n",
            "\n",
            "(7) Exchange\n",
            "Input [2]: [category#18, sum#753]\n",
            "Arguments: hashpartitioning(category#18, 200), ENSURE_REQUIREMENTS, [plan_id=392]\n",
            "\n",
            "(8) HashAggregate\n",
            "Input [2]: [category#18, sum#753]\n",
            "Keys [1]: [category#18]\n",
            "Functions [1]: [sum(value#19)]\n",
            "Aggregate Attributes [1]: [sum(value#19)#691]\n",
            "Results [2]: [category#18, sum(value#19)#691 AS sum_value#686]\n",
            "\n",
            "(9) AdaptiveSparkPlan\n",
            "Output [2]: [category#18, sum_value#686]\n",
            "Arguments: isFinalPlan=false\n",
            "\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan (9)\n",
            "+- HashAggregate (8)\n",
            "   +- Exchange (7)\n",
            "      +- HashAggregate (6)\n",
            "         +- InMemoryTableScan (1)\n",
            "               +- InMemoryRelation (2)\n",
            "                     +- Union (5)\n",
            "                        :- Scan csv  (3)\n",
            "                        +- Scan csv  (4)\n",
            "\n",
            "\n",
            "(1) InMemoryTableScan\n",
            "Output [2]: [category#18, value#19]\n",
            "Arguments: [category#18, value#19]\n",
            "\n",
            "(2) InMemoryRelation\n",
            "Arguments: [id#17, category#18, value#19, text#20], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "\n",
            "(3) Scan csv \n",
            "Output [4]: [id#17, category#18, value#19, text#20]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/mnt/c/Users/daryl/lab1/lab1_dataset_a.csv]\n",
            "ReadSchema: struct<id:int,category:string,value:double,text:string>\n",
            "\n",
            "(4) Scan csv \n",
            "Output [4]: [id#38, category#39, value#40, text#41]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/mnt/c/Users/daryl/lab1/lab1_dataset_b.csv]\n",
            "ReadSchema: struct<id:int,category:string,value:double,text:string>\n",
            "\n",
            "(5) Union\n",
            "\n",
            "(6) HashAggregate\n",
            "Input [2]: [category#18, value#19]\n",
            "Keys [1]: [category#18]\n",
            "Functions [1]: [partial_sum(value#19)]\n",
            "Aggregate Attributes [1]: [sum#924]\n",
            "Results [2]: [category#18, sum#925]\n",
            "\n",
            "(7) Exchange\n",
            "Input [2]: [category#18, sum#925]\n",
            "Arguments: hashpartitioning(category#18, 200), ENSURE_REQUIREMENTS, [plan_id=523]\n",
            "\n",
            "(8) HashAggregate\n",
            "Input [2]: [category#18, sum#925]\n",
            "Keys [1]: [category#18]\n",
            "Functions [1]: [sum(value#19)]\n",
            "Aggregate Attributes [1]: [sum(value#19)#863]\n",
            "Results [2]: [category#18, sum(value#19)#863 AS sum_value#860]\n",
            "\n",
            "(9) AdaptiveSparkPlan\n",
            "Output [2]: [category#18, sum_value#860]\n",
            "Arguments: isFinalPlan=false\n",
            "\n",
            "\n",
            "Open Spark UI at http://localhost:4040 while each job runs and record metrics into lab1_metrics_log.csv\n"
          ]
        }
      ],
      "source": [
        "# Case A: select all columns then aggregate on 'category'\n",
        "all_cols = df.select(\"*\").groupBy(\"category\").agg(F.sum(\"value\").alias(\"sum_value\"))\n",
        "all_cols.explain(\"formatted\")\n",
        "_ = all_cols.count()  # trigger\n",
        "\n",
        "# Case B: minimal projection then aggregate\n",
        "proj = df.select(\"category\",\"value\").groupBy(\"category\").agg(F.sum(\"value\").alias(\"sum_value\"))\n",
        "proj.explain(\"formatted\")\n",
        "_ = proj.count()  # trigger\n",
        "\n",
        "print(\"Open Spark UI at http://localhost:4040 while each job runs and record metrics into lab1_metrics_log.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped.\n"
          ]
        }
      ],
      "source": [
        "spark.stop()\n",
        "print(\"Spark session stopped.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
